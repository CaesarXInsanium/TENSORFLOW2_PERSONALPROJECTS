{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow2_dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNLXcGYb3+rlqtKDy/Y1V6V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CaesarXInsanium/TENSORFLOW2_PERSONALPROJECTS/blob/master/tensorflow2_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GJnwXI2mkkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "\n",
        "def plot_learning_curve(x, scores, epsilons, filename, lines=None):\n",
        "    fig=plt.figure()\n",
        "    ax=fig.add_subplot(111, label=\"1\")\n",
        "    ax2=fig.add_subplot(111, label=\"2\", frame_on=False)\n",
        "\n",
        "    ax.plot(x, epsilons, color=\"C0\")\n",
        "    ax.set_xlabel(\"Training Steps\", color=\"C0\")\n",
        "    ax.set_ylabel(\"Epsilon\", color=\"C0\")\n",
        "    ax.tick_params(axis='x', colors=\"C0\")\n",
        "    ax.tick_params(axis='y', colors=\"C0\")\n",
        "\n",
        "    N = len(scores)\n",
        "    running_avg = np.empty(N)\n",
        "    for t in range(N):\n",
        "\t    running_avg[t] = np.mean(scores[max(0, t-20):(t+1)])\n",
        "\n",
        "    ax2.scatter(x, running_avg, color=\"C1\")\n",
        "    ax2.axes.get_xaxis().set_visible(False)\n",
        "    ax2.yaxis.tick_right()\n",
        "    ax2.set_ylabel('Score', color=\"C1\")\n",
        "    ax2.yaxis.set_label_position('right')\n",
        "    ax2.tick_params(axis='y', colors=\"C1\")\n",
        "\n",
        "    if lines is not None:\n",
        "        for line in lines:\n",
        "            plt.axvline(x=line)\n",
        "\n",
        "    plt.savefig(filename)\n",
        "\n",
        "class RepeatActionAndMaxFrame(gym.Wrapper):\n",
        "    def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,\n",
        "                 fire_first=False):\n",
        "        super(RepeatActionAndMaxFrame, self).__init__(env)\n",
        "        self.repeat = repeat\n",
        "        self.shape = env.observation_space.low.shape\n",
        "        self.frame_buffer = np.zeros_like((2, self.shape))\n",
        "        self.clip_reward = clip_reward\n",
        "        self.no_ops = no_ops\n",
        "        self.fire_first = fire_first\n",
        "\n",
        "    def step(self, action):\n",
        "        t_reward = 0.0\n",
        "        done = False\n",
        "        for i in range(self.repeat):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if self.clip_reward:\n",
        "                reward = np.clip(np.array([reward]), -1, 1)[0]\n",
        "            t_reward += reward\n",
        "            idx = i % 2\n",
        "            self.frame_buffer[idx] = obs\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])\n",
        "        return max_frame, t_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        no_ops = np.random.randint(self.no_ops)+1 if self.no_ops > 0 else 0\n",
        "        for _ in range(no_ops):\n",
        "            _, _, done, _ = self.env.step(0)\n",
        "            if done:\n",
        "                self.env.reset()\n",
        "        if self.fire_first:\n",
        "            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "            obs, _, _, _ = self.env.step(1)\n",
        "\n",
        "        self.frame_buffer = np.zeros_like((2,self.shape))\n",
        "        self.frame_buffer[0] = obs\n",
        "\n",
        "        return obs\n",
        "\n",
        "class PreprocessFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, shape, env=None):\n",
        "        super(PreprocessFrame, self).__init__(env)\n",
        "        self.shape = (shape[2], shape[0], shape[1])\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n",
        "                                    shape=self.shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        new_frame = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "        resized_screen = cv2.resize(new_frame, self.shape[1:],\n",
        "                                    interpolation=cv2.INTER_AREA)\n",
        "        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n",
        "        new_obs = new_obs / 255.0\n",
        "\n",
        "        return new_obs\n",
        "\n",
        "class StackFrames(gym.ObservationWrapper):\n",
        "    def __init__(self, env, repeat):\n",
        "        super(StackFrames, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "                            env.observation_space.low.repeat(repeat, axis=0),\n",
        "                            env.observation_space.high.repeat(repeat, axis=0),\n",
        "                            dtype=np.float32)\n",
        "        self.stack = collections.deque(maxlen=repeat)\n",
        "\n",
        "    def reset(self):\n",
        "        self.stack.clear()\n",
        "        observation = self.env.reset()\n",
        "        for _ in range(self.stack.maxlen):\n",
        "            self.stack.append(observation)\n",
        "\n",
        "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.stack.append(observation)\n",
        "\n",
        "        return np.array(self.stack).reshape(self.observation_space.low.shape)\n",
        "\n",
        "def make_env(env_name, shape=(84,84,1), repeat=4, clip_rewards=False,\n",
        "             no_ops=0, fire_first=False):\n",
        "    env = gym.make(env_name)\n",
        "    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards, no_ops, fire_first)\n",
        "    env = PreprocessFrame(shape, env)\n",
        "    env = StackFrames(env, repeat)\n",
        "\n",
        "    return env\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgHtiWXzG4NM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                     dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
        "                                         dtype=np.float32)\n",
        "\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.uint8)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        terminal = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, terminal\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lja2f0j8uGUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMem(object):\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.state_mem = tf.zeros((self.mem_size, *input_shape), dtype=tf.float32)\n",
        "        self.new_state_mem = tf.zeros((self.mem_size, *input_shape), dtype=tf.float32)\n",
        "        self.action_mem = tf.zeros((self.mem_size), dtype=tf.int32)\n",
        "        self.reward_mem = tf.zeros((self.mem_size), dtype=tf.int32)\n",
        "        self.term_mem = tf.zeros((self.mem_size), dtype=tf.int16)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done: bool):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_mem = tf.concat([self.state_mem[1:], [state]], axis=0)\n",
        "        self.new_state_mem = tf.concat([self.new_state_mem[1:], [state_]], axis=0)\n",
        "        self.action_mem = tf.concat([self.action_mem[1:], [action]], axis=0)\n",
        "        self.reward_mem = tf.concat([self.reward_mem[1:], [int(reward)]], axis=0)\n",
        "        self.reward_mem = tf.concat([self.term_mem[1:], [done]], axis=0)\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        batch = [[i] for i in range(batch_size)]\n",
        "        states = tf.gather_nd(tf.random.shuffle(self.state_mem), batch)\n",
        "        actions = tf.gather_nd(tf.random.shuffle(self.action_mem), batch)\n",
        "        rewards = tf.gather_nd(tf.random.shuffle(self.reward_mem), batch)\n",
        "        states_ = tf.gather_nd(tf.random.shuffle(self.new_state_mem), batch)\n",
        "        terminal = tf.gather_nd(tf.random.shuffle(self.term_mem), batch)\n",
        "\n",
        "        return states, actions, rewards, states_, terminal\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG2yw64TkTFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "\n",
        "#need a gpu\n",
        "\n",
        "class DQN(keras.Model):\n",
        "    def __init__(self,lr,n_actions,name,input_dims,chkpt_dir):\n",
        "        super(DQN, self).__init__()\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name)\n",
        "        #by default conv2D uses format (NWHC): (batch, width,height,channels)\n",
        "        #format ncwh requires a gpu at the moment that is why we are here at google colab\n",
        "        \n",
        "        self.conv1 = keras.layers.Conv2D(input_shape=(-1,*input_dims),\n",
        "            filters=32,kernel_size=8,data_format='channels_first',strides=4,activation='relu')\n",
        "        #need the channels first data format in order to use the shape(32,4,84,84) format NCWH\n",
        "        # but now the problem becomes that i actually need a GPU, which is bad for now but I can live with it\n",
        "        #testing in google colab is positive\n",
        "        self.conv2 = keras.layers.Conv2D(filters=32,kernel_size=4,strides=2,data_format='channels_first',\n",
        "            activation='relu')\n",
        "        self.conv3 = keras.layers.Conv2D(filters=64,kernel_size=3,strides=1,data_format='channels_first',\n",
        "            activation='relu')\n",
        "\n",
        "        self.flat = keras.layers.Flatten()\n",
        "        self.dense1 = keras.layers.Dense(units=512, activation='relu')\n",
        "        self.dense2 = keras.layers.Dense(units=n_actions)\n",
        "\n",
        "        #make prepareations in order to move operatiions to proper device. for local runtime\n",
        "\n",
        "    def call(self, state):\n",
        "\n",
        "        #print('input shape: ',state.shape)\n",
        "        #exit()\n",
        "        conv1 = self.conv1(state)\n",
        "        #print('Conv1 output shape:', conv1.shape)\n",
        "        conv2 = self.conv2(conv1)\n",
        "        #print('Conv2 output shape:', conv2.shape)\n",
        "        conv3 = self.conv3(conv2)\n",
        "        #print('Conv3 output shape:',conv3.shape)\n",
        "        flat = self.flat(conv3)\n",
        "        #print('flat : ', flat.shape)\n",
        "        dense1 = self.dense1(flat)\n",
        "        #print('dense1 shape: ', dense1.shape)\n",
        "        actions = self.dense2(dense1)\n",
        "        #print('actions: ', actions.shape)\n",
        "        return actions\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "      pass\n",
        "      print('Saving weights...')\n",
        "      self.save(self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "      pass\n",
        "      print('loading checkpoint...')\n",
        "      self.load_weights(self.checkpoint_file)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXFZ4OqvmK8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#testing the model\n",
        "#moodel = DQN(0.001, 4, 'name', (4,84,84), 'ccc')\n",
        "#data = tf.random.uniform((32,4,84,84))\n",
        "#moodel.call(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylwgLnXByAPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#env = make_env('PongNoFrameskip-v4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwfILiuBzEVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent(object):\n",
        "    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,\n",
        "                 mem_size, batch_size, eps_min=0.01, eps_dec=5e-7,\n",
        "                 replace=1000, algo=None, env_name=None, chkpt_dir='tmp/dqn'):\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.lr = lr\n",
        "        self.n_actions = n_actions\n",
        "        self.input_dims = input_dims\n",
        "        self.batch_size = batch_size\n",
        "        self.eps_min = eps_min\n",
        "        self.eps_dec = eps_dec\n",
        "        self.replace_target_cnt = replace\n",
        "        self.algo = algo\n",
        "        self.env_name = env_name\n",
        "        self.chkpt_dir = chkpt_dir\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.learn_step_counter = 0\n",
        "\n",
        "        self.memory = ReplayMem(mem_size, input_dims, n_actions)\n",
        "\n",
        "        self.q_eval = DQN(self.lr, self.n_actions,\n",
        "                                    input_dims=self.input_dims,\n",
        "                                    name=self.env_name+'_'+self.algo+'_q_eval',\n",
        "                                    chkpt_dir=self.chkpt_dir)\n",
        "\n",
        "        self.q_next = DQN(self.lr, self.n_actions,\n",
        "                                    input_dims=self.input_dims,\n",
        "                                    name=self.env_name+'_'+self.algo+'_q_next',\n",
        "                                    chkpt_dir=self.chkpt_dir)\n",
        "        \n",
        "    def choose_action(self, observation):\n",
        "        if tf.random.uniform([1]) > self.epsilon:\n",
        "          #take tensor and move to device\n",
        "          #increase rank\n",
        "          actions = self.q_eval.call(tf.expand_dims(observation, axis=0))\n",
        "          #print('model output = ', actions)\n",
        "          \n",
        "          action = tf.argmax(actions, axis=1)\n",
        "          #print('model action type:',type(action))\n",
        "          #print(action)\n",
        "\n",
        "        else:\n",
        "          action = np.random.choice(self.action_space)\n",
        "          #print('random action type: ', type(action))\n",
        "          #print(action)\n",
        "          #the enviroment wants an integer as input for an action\n",
        "\n",
        "        return action\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        self.memory.store_transition(state,action,reward,state_,done)\n",
        "\n",
        "    def sample_memory(self):\n",
        "        states,actions,rewards,new_states,dones = self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        return states, actions, rewards, new_states,dones\n",
        "\n",
        "    def replace_target_network(self):\n",
        "        if self.learn_step_counter & self.replace_target_cnt ==0:\n",
        "            self.q_next = self.q_eval\n",
        "\n",
        "    def decrement_epsilon(self):\n",
        "        self.epsilon = self.epsilon - self.eps_dec \\\n",
        "                           if self.epsilon > self.eps_min else self.eps_min\n",
        "        \n",
        "    def save_models(self):\n",
        "        self.q_eval.save_checkpoint()\n",
        "        self.q_next.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        self.q_eval.load_checkpoint()\n",
        "        self.q_next.load_checkpoint()\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "    \n",
        "        self.replace_target_network()\n",
        "        states,actions,rewards,states_,dones = self.sample_memory()\n",
        "        optimizer = keras.optimizers.RMSprop(learning_rate=self.lr)\n",
        "        indices = tf.range(self.batch_size)\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            q_pred = tf.gather_nd(self.q_eval.call(states), list(zip(indices, actions)))\n",
        "            q_next = tf.math.reduce_max(self.q_next.call(states_), axis=1)\n",
        "        #is the code below even doing anything?\n",
        "        #q_next[dones] = 0.0\n",
        "            q_target = rewards + self.gamma * q_next\n",
        "            loss = keras.losses.MSE(q_target, q_pred)\n",
        "        gradient = tape.gradient(loss, self.q_eval.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradient, self.q_eval.trainable_variables))\n",
        "        self.decrement_epsilon()\n",
        "\n",
        "      \n",
        "  \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFiouREvE76Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(num_games= 10,load_checkpoint=False, env_name='PongNoFrameskip-v4'):\n",
        "    env  = make_env(env_name)\n",
        "    best_score = -np.inf\n",
        "\n",
        "    agent = DQNAgent(gamma=0.99, epsilon=1.0,lr=0.0001,input_dims=(env.observation_space.shape),\n",
        "                     n_actions=env.action_space.n, mem_size=20000, eps_min=0.1, batch_size=32,replace=1000,\n",
        "                     eps_dec=1e-5, chkpt_dir='models/',algo='DQNAgent', env_name=env_name)\n",
        "    if load_checkpoint:\n",
        "          agent.load_models()\n",
        "    fname = agent.algo + '_' + agent.env_name + '_lr' + str(agent.lr) +'_' + str(num_games) + 'games'\n",
        "    try:\n",
        "        os.mkdir('plots')\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    figure_file = 'plots/' + fname + '.png'\n",
        "    n_steps = 0\n",
        "    scores, eps_history,steps_array = [], [], []\n",
        "\n",
        "    for i in range(num_games):\n",
        "          done = False\n",
        "          observation = env.reset()\n",
        "          score = 0\n",
        "          while not done:\n",
        "                action = agent.choose_action(observation)\n",
        "                observation_,reward,done,info = env.step(action)\n",
        "                score += reward\n",
        "                if not load_checkpoint:\n",
        "                      agent.store_transition(observation, action, reward,observation_, int(done))\n",
        "                      agent.learn()\n",
        "\n",
        "                observation = observation_\n",
        "          scores.append(score)\n",
        "          steps_array.append(n_steps)\n",
        "          avg_score = np.mean(scores[-100:])\n",
        "          print('episode: ', i,'score: ', score,\n",
        "             ' average score %.1f' % avg_score, 'best score %.2f' % best_score,\n",
        "            'epsilon %.2f' % agent.epsilon, 'steps', n_steps)\n",
        "          \n",
        "          if avg_score > best_score:\n",
        "            #if not load_checkpoint:\n",
        "            #    agent.save_models()\n",
        "              best_score = avg_score\n",
        "\n",
        "          eps_history.append(agent.epsilon)\n",
        "          if load_checkpoint and n_steps >= 18000:\n",
        "              break\n",
        "\n",
        "    x = [i+1 for i in range(len(scores))]\n",
        "    plot_learning_curve(steps_array, scores, eps_history, figure_file)\n",
        "\n",
        "      \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSiymtrv48N4",
        "colab_type": "code",
        "outputId": "314d9050-c3f0-4018-cae4-02cc2f2d415f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main(300)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n",
            "<class 'float'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-1e4d9bf65ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-d3b00db7a0a0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_games, load_checkpoint, env_name)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                       \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                       \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-7735a93f888b>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m#is the code below even doing anything?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m#q_next[dones] = 0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mq_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a int16 tensor but is a float tensor [Op:AddV2]"
          ]
        }
      ]
    }
  ]
}